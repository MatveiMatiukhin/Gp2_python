# Gp2_python
## Описание репозитория
Данный репозиторий создан для проекта Gp2 на тему  
- скрапинг
- парсинг через API
- EDA датасета, состоящего из спаршенных данных  
Репозиторий состоит из директории src, включающей в себя скрипты с кодом парсинга и EDA, директории misc, предполагающей нахождение в ней файлов фотографий, директории datasets, где находятся основной, вспомогательный и итоговый датасеты, а также из файлов gitignore и Readme
## Этапы выполнения проекта
### 1. Парсинг данных
#### 1.1 Скрапинг
1. Сначала устанавливаем окружение и необходимые зависимости, чтобы обеспечить запуск ноутбука и доступ к нужным библиотекам для парсинга веб-страниц и обработки данных. Это включает установку соответствующих версий Python и пакетов через менеджер зависимостей (например, pip/requirements.txt или Poetry). 
2. Затем импортируем нужные библиотеки: для работы с веб-страницами — Selenium или requests/BeautifulSoup, для обработки данных — pandas, numpy, а для сохранения результатов — pandas DataFrame и экспорт в CSV/Excel. 
3. Настраиваем веб-драйвер (если используется Selenium) или целевые URL-адреса. Указываем стратегию ожидания элементов (explicit waits) и опции браузера (headless режим, отключение рекламы и т.д.). 
4. Загружаем целевые страницы или API-эндпоинты, проходя по нужной навигации и/или формируя запросы с параметрами.Сохраняем полученные HTML 
5. Парсим данные: извлекаем необходимую информацию (например, названия треков, исполнителей, метаданные) с использованием селекторов BeautifulSoup, и Selenium. Приводим данные к табличной форме: столбцы разделены на отдельные колонки (например, title и artist). 
6. Обрабатываем данные: очищаем текст, приводим к единому формату, обрабатываем пропуски, приводим типы данных, выполняем базовую валидацию. 
7. Сохраняем результаты в локальные файлы: CSV в указанный каталог.
#### 1.2 Парсинг через API
1. Устанавливаем нужные версии Python и устанавливаем зависимости (через requirements.txt или poetry).

2. Импортируем библиотеки для HTTP/парсинга: requests или aiohttp, BeautifulSoup или lxml; для работы с данными — pandas, numpy; для сохранения — pandas (to_csv/to_excel).

3. Загружаем и применяем конфигурации: целевые URL, параметры запросов, заголовки, тайм-ауты и политики повторных попыток.

4. Выполняем HTTP-запросы к целевым источникам (страницы или API).

5. При использовании API обрабатываем авторизацию (API-ключи, токены) и параметры запроса.

6. Извлекаем целевые поля

7. Приводим данные к единообразному формату: нормализация строк, разделение полей, приведение типов (числа, даты).

8. Выполняем базовую валидацию и устранение дубликатов.

9. Сохранение промежуточных и итоговых данных

10. Реализованы обработчики исключений на сетевые ошибки, ошибки парсинга и тайм-ауты.

11. При неудачных попытках выполняем повторные запросы с экспоненциальной задержкой.

12. Формирование итогового набора данных

13. Объединяем данные в единый DataFrame, выполняем агрегацию или сводку по нужным полям.

14. Сохраняем итоговый CSV

15. Включено логирование уровня DEBUG/INFO для прозрачности выполнения.
### 2. Соединение данных в один дата фрейм
1. Устанавливаем необходимые библиотеки и зависимости, которые потребуются для загрузки данных и обработки таблиц
2. 